<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <link rel="icon" href="https://github.com/CX9898/CX9898.github.io/blob/main/backup/img/CX.jpg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="![论文封面](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/论文封面.png)

# [论文笔记]深度学习中N:M稀疏权重的高效GPU内核

**EFFICIENT GPU KERNELS FOR N:M-SPARSE WEIGHTS IN DEEP LEARNING**

论文于2023年发表在Sixth Conference on Machine Learning and Systems · Miami (MLSys 23).

在深度学习领域中,N:M稀疏性越来越受欢迎. 但因为缺乏针对各种稀疏比的通用GPU kernel库,论文介绍了一个**高效GPU kernel库: nmSPARSE. 用于具有N:M稀疏权重的神经网络的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).**

论文主要探讨针对N:M稀疏性的稀疏矩阵-向量乘法和稀疏矩阵-矩阵乘法的GPU加速. 下面先介绍什么是权重剪枝和N:M稀疏性.

***

## Weight Pruning

为了减少深度神经网络(DNN)的模型大小并加速模型推理, 权重剪枝(Weight Pruning)算法在学术界和工业界得到广泛的研究.

**权重剪枝的目的是找到并去除对模型精度影响不大的冗余权重. 从而直接减少了模型的内存占用和计算规模.**

![如图所示,图中占比重要的权重被保留,冗余的权重被去除.在保留模型精度的同时降低了计算规模](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/稀疏剪枝.png)

如图所示, 图中占比重要的权重被保留, 冗余的权重被去除. 在保留模型精度的同时降低了计算规模.

> 通过调整权重剪枝(Weight Pruning)算法可以将权值修剪为N:M稀疏模式, 随着权值被修剪为稀疏后, DNN推理中最频繁和耗时的稠密矩阵-向量乘法(GEMV)和稠密矩阵-矩阵乘法(GEMM)变为了具有N:M稀疏性的稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM). 那么什么是N:M稀疏性呢?

***

## N:M sparsity

N:M稀疏性可以为深度学习提供高模型精度和计算效率.

**N:M稀疏性本质上对非零权重施加了平衡分布,具体为每连续的M个权值中,只有N个权值不为零.**

![三种稀疏模式](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/三种NM稀疏模式.png)

图上展示了三种稀疏模式, 元素型, 矢量型和块型. 图中蓝色方块代表非零值, 白色方块代表0. 上面一列是无约束分布的数据, 下面一列是N:M平衡分布的数据. 图中稀疏比为1:4. 左下角的图表示垂直的每四个权值中有一个非零权值.

> 注意,N:M分布是沿着矩阵乘法中的降维k分布的
N:M稀疏性只限制了非零元素的局部分布,每个M大小的窗口中的分布不受限制.所以对模型精度的影响很小.同时N:M稀疏性在GPU上实现高效并行执行方面有很大的潜力.

***

## N:M sparsity压缩表示

稀疏矩阵有各种压缩表示格式,如CSC,CSR,COO等.

论文中给出专用于nmSPARSE中N:M稀疏模式的压缩表示格式.

![EW-/VW-/BW-N:M sparsity的数据压缩表示](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/NM稀疏模式的压缩表示格式.png)

EW-/VW-/BW-N:M sparsity的数据压缩表示.

如图,将非零数据按竖直方向压缩后,创建一个大小相同的位置数组记录对应非零数据在原矩阵中M窗口中的位置索引. 并记录稀疏模式和稀疏比.

***

## nmSPARSE库

论文介绍的nmSPARSE是一个高效的GPU kernel库,用于N:M稀疏权重的神经网络中的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).

并基于ASP算法(由Nvidia开发用于生成稀疏网络的开源Pruning库), 做出三个扩展:
1. 支持任意N:M设置. 意味着支持生成任意N:M稀疏比
2. 扩展ASP支持nmSPARSE的VW/BW-N:M稀疏模式
3. 进一步启用分层稀疏比配置

值得一提的是**在实现nmSPARSE的SpMV和SpMM kernel时利用了共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).**

下面介绍什么是共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).

***

### 共享内存:无冲突访问(conflict-free access)

> 共享内存是一个可以被同时访问的一维地址空间.
> 共享内存中被分为32个同样大小的存储体(bank)，对应线程束中32个线程.

**在共享内存中当多个地址请求落在相同的内存库的不同地址中时， 就会发生存储体冲突(bank conflict)， 这会导致请求被重复执行, 从而降低带宽.**

![多个线程同时访问同一个bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/多个线程同时访问同一个bank(bank_conflict).png)

多个线程同时访问同一个bank产生bank conflict. 图片来自谭升的博客.

> 这里对于bank conflict不做深入的介绍,具体可以参考这个链接的文章:[共享内存之bank冲突](https://segmentfault.com/a/1190000007533157)

**相反,如果warp中的32个线程的内存访问映射到32个不同的存储体,那么可以同时提供服务,不会产生bank冲突,这种模式也被称为无冲突访问模式(conflict-free access)**

![每个线程都访问不同的bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/无冲突访问模式.png)

最理想的访问模式, 每个线程都访问不同的bank. 图片来自谭升的博客.

当然不规则访问的同时访问不同的bank也属于无冲突访问模式(conflict-free access)

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/不规则访问但也不冲突.png)

不规则访问的无冲突访问模式(conflict-free access). 图片来自谭升的博客.

***

### 共享内存:无冲突广播访问(conflict-free broadcast access)

**当一个warp中的多个线程访问同一个bank,但是使用的是完全相同的地址时,可以通过硬件支持的广播机制来提供服务,不会出现存储体冲突(bank conflict)**.这种访问模式称为无冲突广播访问模式(conflict-free broadcast access).

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同的bank.png)

> 上图两条红色的线访问了同一个bank1的不同地址, 出现了bank冲突.

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同bank的相同地址.png)

> 如上图, 虽然有两个线程都访问了同一个bank1,但是访问的是完全相同的地址.这种访问模式就称为无冲突广播访问模式(conflict-free broadcast access).

***

### nmSPARSE: SpMV kernel的无冲突访问

1. nmSPARSE的SpMV kernel进一步将每个稀疏列划分为大小为M的子列. 
2. 每个线程计算稀疏矩阵的一个子列的点积.
3. 因为每M个元素中只有N个非零元素,所以每个线程的工作负载是平衡的.
4. 稠密向量A则从全局内存加载到共享内存中,并且根据每个子列需要的向量划分到不同的内存块中.
5. 不同线程只需要访问不同的内存块,消除了bank冲突.

![各个线程访问不同的储存库,例如线程T0访问bank0,线程T1访问bank1](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMV.png)

***

### nmSPARSE: SpMM kernel的无冲突广播访问

nmSPARSE中的SpMM kernel将稠密矩阵A储存在共享内存中,将线程映射到稀疏矩阵B中的列来实现对A中所需元素的访问

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMM.png)

1. 通过将稠密A矩阵按行储存在共享内存中,如图A矩阵一行有32个元素,一行的元素刚好可以对应储存同在一个bank.
2. 每个线程负责稀疏B矩阵的每一列.
3. 由于N:M稀疏模式按列的内部平衡分布,所以每个线程的工作负载是平衡的. 
4. 在同一个warp中的两个线程访问同一块内存中完全相同的地址,可以运用广播机制完美解决这个冲突.

***

## nmSPARSE性能测试

通过与当前最先进的稠密和稀疏库以及DNN编译器进行比较，在操作基准和端到端模型上评估nmSPARSE内核。">
<meta property="og:title" content="[论文笔记]深度学习中N:M稀疏权重的高效GPU内核">
<meta property="og:description" content="![论文封面](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/论文封面.png)

# [论文笔记]深度学习中N:M稀疏权重的高效GPU内核

**EFFICIENT GPU KERNELS FOR N:M-SPARSE WEIGHTS IN DEEP LEARNING**

论文于2023年发表在Sixth Conference on Machine Learning and Systems · Miami (MLSys 23).

在深度学习领域中,N:M稀疏性越来越受欢迎. 但因为缺乏针对各种稀疏比的通用GPU kernel库,论文介绍了一个**高效GPU kernel库: nmSPARSE. 用于具有N:M稀疏权重的神经网络的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).**

论文主要探讨针对N:M稀疏性的稀疏矩阵-向量乘法和稀疏矩阵-矩阵乘法的GPU加速. 下面先介绍什么是权重剪枝和N:M稀疏性.

***

## Weight Pruning

为了减少深度神经网络(DNN)的模型大小并加速模型推理, 权重剪枝(Weight Pruning)算法在学术界和工业界得到广泛的研究.

**权重剪枝的目的是找到并去除对模型精度影响不大的冗余权重. 从而直接减少了模型的内存占用和计算规模.**

![如图所示,图中占比重要的权重被保留,冗余的权重被去除.在保留模型精度的同时降低了计算规模](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/稀疏剪枝.png)

如图所示, 图中占比重要的权重被保留, 冗余的权重被去除. 在保留模型精度的同时降低了计算规模.

> 通过调整权重剪枝(Weight Pruning)算法可以将权值修剪为N:M稀疏模式, 随着权值被修剪为稀疏后, DNN推理中最频繁和耗时的稠密矩阵-向量乘法(GEMV)和稠密矩阵-矩阵乘法(GEMM)变为了具有N:M稀疏性的稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM). 那么什么是N:M稀疏性呢?

***

## N:M sparsity

N:M稀疏性可以为深度学习提供高模型精度和计算效率.

**N:M稀疏性本质上对非零权重施加了平衡分布,具体为每连续的M个权值中,只有N个权值不为零.**

![三种稀疏模式](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/三种NM稀疏模式.png)

图上展示了三种稀疏模式, 元素型, 矢量型和块型. 图中蓝色方块代表非零值, 白色方块代表0. 上面一列是无约束分布的数据, 下面一列是N:M平衡分布的数据. 图中稀疏比为1:4. 左下角的图表示垂直的每四个权值中有一个非零权值.

> 注意,N:M分布是沿着矩阵乘法中的降维k分布的
N:M稀疏性只限制了非零元素的局部分布,每个M大小的窗口中的分布不受限制.所以对模型精度的影响很小.同时N:M稀疏性在GPU上实现高效并行执行方面有很大的潜力.

***

## N:M sparsity压缩表示

稀疏矩阵有各种压缩表示格式,如CSC,CSR,COO等.

论文中给出专用于nmSPARSE中N:M稀疏模式的压缩表示格式.

![EW-/VW-/BW-N:M sparsity的数据压缩表示](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/NM稀疏模式的压缩表示格式.png)

EW-/VW-/BW-N:M sparsity的数据压缩表示.

如图,将非零数据按竖直方向压缩后,创建一个大小相同的位置数组记录对应非零数据在原矩阵中M窗口中的位置索引. 并记录稀疏模式和稀疏比.

***

## nmSPARSE库

论文介绍的nmSPARSE是一个高效的GPU kernel库,用于N:M稀疏权重的神经网络中的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).

并基于ASP算法(由Nvidia开发用于生成稀疏网络的开源Pruning库), 做出三个扩展:
1. 支持任意N:M设置. 意味着支持生成任意N:M稀疏比
2. 扩展ASP支持nmSPARSE的VW/BW-N:M稀疏模式
3. 进一步启用分层稀疏比配置

值得一提的是**在实现nmSPARSE的SpMV和SpMM kernel时利用了共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).**

下面介绍什么是共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).

***

### 共享内存:无冲突访问(conflict-free access)

> 共享内存是一个可以被同时访问的一维地址空间.
> 共享内存中被分为32个同样大小的存储体(bank)，对应线程束中32个线程.

**在共享内存中当多个地址请求落在相同的内存库的不同地址中时， 就会发生存储体冲突(bank conflict)， 这会导致请求被重复执行, 从而降低带宽.**

![多个线程同时访问同一个bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/多个线程同时访问同一个bank(bank_conflict).png)

多个线程同时访问同一个bank产生bank conflict. 图片来自谭升的博客.

> 这里对于bank conflict不做深入的介绍,具体可以参考这个链接的文章:[共享内存之bank冲突](https://segmentfault.com/a/1190000007533157)

**相反,如果warp中的32个线程的内存访问映射到32个不同的存储体,那么可以同时提供服务,不会产生bank冲突,这种模式也被称为无冲突访问模式(conflict-free access)**

![每个线程都访问不同的bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/无冲突访问模式.png)

最理想的访问模式, 每个线程都访问不同的bank. 图片来自谭升的博客.

当然不规则访问的同时访问不同的bank也属于无冲突访问模式(conflict-free access)

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/不规则访问但也不冲突.png)

不规则访问的无冲突访问模式(conflict-free access). 图片来自谭升的博客.

***

### 共享内存:无冲突广播访问(conflict-free broadcast access)

**当一个warp中的多个线程访问同一个bank,但是使用的是完全相同的地址时,可以通过硬件支持的广播机制来提供服务,不会出现存储体冲突(bank conflict)**.这种访问模式称为无冲突广播访问模式(conflict-free broadcast access).

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同的bank.png)

> 上图两条红色的线访问了同一个bank1的不同地址, 出现了bank冲突.

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同bank的相同地址.png)

> 如上图, 虽然有两个线程都访问了同一个bank1,但是访问的是完全相同的地址.这种访问模式就称为无冲突广播访问模式(conflict-free broadcast access).

***

### nmSPARSE: SpMV kernel的无冲突访问

1. nmSPARSE的SpMV kernel进一步将每个稀疏列划分为大小为M的子列. 
2. 每个线程计算稀疏矩阵的一个子列的点积.
3. 因为每M个元素中只有N个非零元素,所以每个线程的工作负载是平衡的.
4. 稠密向量A则从全局内存加载到共享内存中,并且根据每个子列需要的向量划分到不同的内存块中.
5. 不同线程只需要访问不同的内存块,消除了bank冲突.

![各个线程访问不同的储存库,例如线程T0访问bank0,线程T1访问bank1](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMV.png)

***

### nmSPARSE: SpMM kernel的无冲突广播访问

nmSPARSE中的SpMM kernel将稠密矩阵A储存在共享内存中,将线程映射到稀疏矩阵B中的列来实现对A中所需元素的访问

![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMM.png)

1. 通过将稠密A矩阵按行储存在共享内存中,如图A矩阵一行有32个元素,一行的元素刚好可以对应储存同在一个bank.
2. 每个线程负责稀疏B矩阵的每一列.
3. 由于N:M稀疏模式按列的内部平衡分布,所以每个线程的工作负载是平衡的. 
4. 在同一个warp中的两个线程访问同一块内存中完全相同的地址,可以运用广播机制完美解决这个冲突.

***

## nmSPARSE性能测试

通过与当前最先进的稠密和稀疏库以及DNN编译器进行比较，在操作基准和端到端模型上评估nmSPARSE内核。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://CX9898.github.io/post/%5B-lun-wen-bi-ji-%5D-shen-du-xue-xi-zhong-N-M-xi-shu-quan-zhong-de-gao-xiao-GPU-nei-he.html">
<meta property="og:image" content="https://github.com/CX9898/CX9898.github.io/blob/main/backup/img/CX.jpg">
<title>[论文笔记]深度学习中N:M稀疏权重的高效GPU内核</title>


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">[论文笔记]深度学习中N:M稀疏权重的高效GPU内核</h1>
<div class="title-right">
    <a href="https://CX9898.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/CX9898/CX9898.github.io/issues/1" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E8%AE%BA%E6%96%87%E5%B0%81%E9%9D%A2.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E8%AE%BA%E6%96%87%E5%B0%81%E9%9D%A2.png" alt="论文封面" style="max-width: 100%;"></a></p>
<h1>[论文笔记]深度学习中N:M稀疏权重的高效GPU内核</h1>
<p><strong>EFFICIENT GPU KERNELS FOR N:M-SPARSE WEIGHTS IN DEEP LEARNING</strong></p>
<p>论文于2023年发表在Sixth Conference on Machine Learning and Systems · Miami (MLSys 23).</p>
<p>在深度学习领域中,N:M稀疏性越来越受欢迎. 但因为缺乏针对各种稀疏比的通用GPU kernel库,论文介绍了一个<strong>高效GPU kernel库: nmSPARSE. 用于具有N:M稀疏权重的神经网络的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).</strong></p>
<p>论文主要探讨针对N:M稀疏性的稀疏矩阵-向量乘法和稀疏矩阵-矩阵乘法的GPU加速. 下面先介绍什么是权重剪枝和N:M稀疏性.</p>
<hr>
<h2>Weight Pruning</h2>
<p>为了减少深度神经网络(DNN)的模型大小并加速模型推理, 权重剪枝(Weight Pruning)算法在学术界和工业界得到广泛的研究.</p>
<p><strong>权重剪枝的目的是找到并去除对模型精度影响不大的冗余权重. 从而直接减少了模型的内存占用和计算规模.</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E7%A8%80%E7%96%8F%E5%89%AA%E6%9E%9D.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E7%A8%80%E7%96%8F%E5%89%AA%E6%9E%9D.png" alt="如图所示,图中占比重要的权重被保留,冗余的权重被去除.在保留模型精度的同时降低了计算规模" style="max-width: 100%;"></a></p>
<p>如图所示, 图中占比重要的权重被保留, 冗余的权重被去除. 在保留模型精度的同时降低了计算规模.</p>
<blockquote>
<p>通过调整权重剪枝(Weight Pruning)算法可以将权值修剪为N:M稀疏模式, 随着权值被修剪为稀疏后, DNN推理中最频繁和耗时的稠密矩阵-向量乘法(GEMV)和稠密矩阵-矩阵乘法(GEMM)变为了具有N:M稀疏性的稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM). 那么什么是N:M稀疏性呢?</p>
</blockquote>
<hr>
<h2>N:M sparsity</h2>
<p>N:M稀疏性可以为深度学习提供高模型精度和计算效率.</p>
<p><strong>N:M稀疏性本质上对非零权重施加了平衡分布,具体为每连续的M个权值中,只有N个权值不为零.</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%89%E7%A7%8DNM%E7%A8%80%E7%96%8F%E6%A8%A1%E5%BC%8F.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%89%E7%A7%8DNM%E7%A8%80%E7%96%8F%E6%A8%A1%E5%BC%8F.png" alt="三种稀疏模式" style="max-width: 100%;"></a></p>
<p>图上展示了三种稀疏模式, 元素型, 矢量型和块型. 图中蓝色方块代表非零值, 白色方块代表0. 上面一列是无约束分布的数据, 下面一列是N:M平衡分布的数据. 图中稀疏比为1:4. 左下角的图表示垂直的每四个权值中有一个非零权值.</p>
<blockquote>
<p>注意,N:M分布是沿着矩阵乘法中的降维k分布的<br>
N:M稀疏性只限制了非零元素的局部分布,每个M大小的窗口中的分布不受限制.所以对模型精度的影响很小.同时N:M稀疏性在GPU上实现高效并行执行方面有很大的潜力.</p>
</blockquote>
<hr>
<h2>N:M sparsity压缩表示</h2>
<p>稀疏矩阵有各种压缩表示格式,如CSC,CSR,COO等.</p>
<p>论文中给出专用于nmSPARSE中N:M稀疏模式的压缩表示格式.</p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/NM%E7%A8%80%E7%96%8F%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%8E%8B%E7%BC%A9%E8%A1%A8%E7%A4%BA%E6%A0%BC%E5%BC%8F.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/NM%E7%A8%80%E7%96%8F%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%8E%8B%E7%BC%A9%E8%A1%A8%E7%A4%BA%E6%A0%BC%E5%BC%8F.png" alt="EW-/VW-/BW-N:M sparsity的数据压缩表示" style="max-width: 100%;"></a></p>
<p>EW-/VW-/BW-N:M sparsity的数据压缩表示.</p>
<p>如图,将非零数据按竖直方向压缩后,创建一个大小相同的位置数组记录对应非零数据在原矩阵中M窗口中的位置索引. 并记录稀疏模式和稀疏比.</p>
<hr>
<h2>nmSPARSE库</h2>
<p>论文介绍的nmSPARSE是一个高效的GPU kernel库,用于N:M稀疏权重的神经网络中的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).</p>
<p>并基于ASP算法(由Nvidia开发用于生成稀疏网络的开源Pruning库), 做出三个扩展:</p>
<ol>
<li>支持任意N:M设置. 意味着支持生成任意N:M稀疏比</li>
<li>扩展ASP支持nmSPARSE的VW/BW-N:M稀疏模式</li>
<li>进一步启用分层稀疏比配置</li>
</ol>
<p>值得一提的是<strong>在实现nmSPARSE的SpMV和SpMM kernel时利用了共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).</strong></p>
<p>下面介绍什么是共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).</p>
<hr>
<h3>共享内存:无冲突访问(conflict-free access)</h3>
<blockquote>
<p>共享内存是一个可以被同时访问的一维地址空间.<br>
共享内存中被分为32个同样大小的存储体(bank)，对应线程束中32个线程.</p>
</blockquote>
<p><strong>在共享内存中当多个地址请求落在相同的内存库的不同地址中时， 就会发生存储体冲突(bank conflict)， 这会导致请求被重复执行, 从而降低带宽.</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E5%A4%9A%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%97%B6%E8%AE%BF%E9%97%AE%E5%90%8C%E4%B8%80%E4%B8%AAbank(bank_conflict).png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E5%A4%9A%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%97%B6%E8%AE%BF%E9%97%AE%E5%90%8C%E4%B8%80%E4%B8%AAbank(bank_conflict).png" alt="多个线程同时访问同一个bank. 图片来自谭升的博客" style="max-width: 100%;"></a></p>
<p>多个线程同时访问同一个bank产生bank conflict. 图片来自谭升的博客.</p>
<blockquote>
<p>这里对于bank conflict不做深入的介绍,具体可以参考这个链接的文章:<a href="https://segmentfault.com/a/1190000007533157" rel="nofollow">共享内存之bank冲突</a></p>
</blockquote>
<p><strong>相反,如果warp中的32个线程的内存访问映射到32个不同的存储体,那么可以同时提供服务,不会产生bank冲突,这种模式也被称为无冲突访问模式(conflict-free access)</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E6%97%A0%E5%86%B2%E7%AA%81%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E6%97%A0%E5%86%B2%E7%AA%81%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F.png" alt="每个线程都访问不同的bank. 图片来自谭升的博客" style="max-width: 100%;"></a></p>
<p>最理想的访问模式, 每个线程都访问不同的bank. 图片来自谭升的博客.</p>
<p>当然不规则访问的同时访问不同的bank也属于无冲突访问模式(conflict-free access)</p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%8D%E8%A7%84%E5%88%99%E8%AE%BF%E9%97%AE%E4%BD%86%E4%B9%9F%E4%B8%8D%E5%86%B2%E7%AA%81.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%8D%E8%A7%84%E5%88%99%E8%AE%BF%E9%97%AE%E4%BD%86%E4%B9%9F%E4%B8%8D%E5%86%B2%E7%AA%81.png" alt="" style="max-width: 100%;"></a></p>
<p>不规则访问的无冲突访问模式(conflict-free access). 图片来自谭升的博客.</p>
<hr>
<h3>共享内存:无冲突广播访问(conflict-free broadcast access)</h3>
<p><strong>当一个warp中的多个线程访问同一个bank,但是使用的是完全相同的地址时,可以通过硬件支持的广播机制来提供服务,不会出现存储体冲突(bank conflict)</strong>.这种访问模式称为无冲突广播访问模式(conflict-free broadcast access).</p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%A4%E4%B8%AA%E7%BA%BF%E7%A8%8B%E8%AE%BF%E9%97%AE%E7%9B%B8%E5%90%8C%E7%9A%84bank.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%A4%E4%B8%AA%E7%BA%BF%E7%A8%8B%E8%AE%BF%E9%97%AE%E7%9B%B8%E5%90%8C%E7%9A%84bank.png" alt="" style="max-width: 100%;"></a></p>
<blockquote>
<p>上图两条红色的线访问了同一个bank1的不同地址, 出现了bank冲突.</p>
</blockquote>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%A4%E4%B8%AA%E7%BA%BF%E7%A8%8B%E8%AE%BF%E9%97%AE%E7%9B%B8%E5%90%8Cbank%E7%9A%84%E7%9B%B8%E5%90%8C%E5%9C%B0%E5%9D%80.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/%E4%B8%A4%E4%B8%AA%E7%BA%BF%E7%A8%8B%E8%AE%BF%E9%97%AE%E7%9B%B8%E5%90%8Cbank%E7%9A%84%E7%9B%B8%E5%90%8C%E5%9C%B0%E5%9D%80.png" alt="" style="max-width: 100%;"></a></p>
<blockquote>
<p>如上图, 虽然有两个线程都访问了同一个bank1,但是访问的是完全相同的地址.这种访问模式就称为无冲突广播访问模式(conflict-free broadcast access).</p>
</blockquote>
<hr>
<h3>nmSPARSE: SpMV kernel的无冲突访问</h3>
<ol>
<li>nmSPARSE的SpMV kernel进一步将每个稀疏列划分为大小为M的子列.</li>
<li>每个线程计算稀疏矩阵的一个子列的点积.</li>
<li>因为每M个元素中只有N个非零元素,所以每个线程的工作负载是平衡的.</li>
<li>稠密向量A则从全局内存加载到共享内存中,并且根据每个子列需要的向量划分到不同的内存块中.</li>
<li>不同线程只需要访问不同的内存块,消除了bank冲突.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE_SpMV.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE_SpMV.png" alt="各个线程访问不同的储存库,例如线程T0访问bank0,线程T1访问bank1" style="max-width: 100%;"></a></p>
<hr>
<h3>nmSPARSE: SpMM kernel的无冲突广播访问</h3>
<p>nmSPARSE中的SpMM kernel将稠密矩阵A储存在共享内存中,将线程映射到稀疏矩阵B中的列来实现对A中所需元素的访问</p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE_SpMM.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE_SpMM.png" alt="" style="max-width: 100%;"></a></p>
<ol>
<li>通过将稠密A矩阵按行储存在共享内存中,如图A矩阵一行有32个元素,一行的元素刚好可以对应储存同在一个bank.</li>
<li>每个线程负责稀疏B矩阵的每一列.</li>
<li>由于N:M稀疏模式按列的内部平衡分布,所以每个线程的工作负载是平衡的.</li>
<li>在同一个warp中的两个线程访问同一块内存中完全相同的地址,可以运用广播机制完美解决这个冲突.</li>
</ol>
<hr>
<h2>nmSPARSE性能测试</h2>
<p>通过与当前最先进的稠密和稀疏库以及DNN编译器进行比较，在操作基准和端到端模型上评估nmSPARSE内核。</p>
<p><strong>研究结果如下：</strong></p>
<ol>
<li>nmSPARSE的EW-N:M内核在SpMV和SpMM算子上分别比最快的基准实现了5.2倍和2.1倍的加速。</li>
<li>随着粒度的增加，nmSPARSE内核在SpMM算子上可以比最快的基准实现高达6.0倍的加速。</li>
<li>对Transformer的端到端研究表明，nmSPARSE优于其他基准。</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE%E5%9C%A8CUDA%E6%A0%B8%E5%BF%83%E4%B8%8A%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84SpMV%E7%AE%97%E5%AD%90%E7%9A%84%E5%8A%A0%E9%80%9F%E7%BB%93%E6%9E%9C.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE%E5%9C%A8CUDA%E6%A0%B8%E5%BF%83%E4%B8%8A%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84SpMV%E7%AE%97%E5%AD%90%E7%9A%84%E5%8A%A0%E9%80%9F%E7%BB%93%E6%9E%9C.png" alt="nmSPARSE在CUDA核心上针对不同大小的SpMV算子的加速结果" style="max-width: 100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE%E5%9C%A8CUDA%E6%A0%B8%E5%BF%83%E4%B8%8A%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84SpMM%E7%AE%97%E5%AD%90%E5%9C%A850%25,75%25,90%25%E7%A8%80%E7%96%8F%E7%8E%87%E4%B8%8B%E7%9A%84%E5%8A%A0%E9%80%9F%E7%BB%93%E6%9E%9C.png"><img src="/img/%5B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADNM%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E7%9A%84%E9%AB%98%E6%95%88GPU%E5%86%85%E6%A0%B8/nmSPARSE%E5%9C%A8CUDA%E6%A0%B8%E5%BF%83%E4%B8%8A%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84SpMM%E7%AE%97%E5%AD%90%E5%9C%A850%25,75%25,90%25%E7%A8%80%E7%96%8F%E7%8E%87%E4%B8%8B%E7%9A%84%E5%8A%A0%E9%80%9F%E7%BB%93%E6%9E%9C.png" alt="nmSPARSE在CUDA核心上对不同大小的SpMM算子在50%,75%,90%稀疏率下的加速结果" style="max-width: 100%;"></a></p>
<p>nmSPARSE在CUDA核心上对不同大小的SpMM算子在50%,75%,90%稀疏率下的加速结果</p>
<hr>
<h2>结语</h2>
<p>论文结合GPU共享内存的优势和N:M稀疏性的平衡分布,设计并实现了调度内存请求的SpMV内核和SpMM内核, 以消除共享内存中的bank冲突. 解决了稀疏矩阵乘法中不规则的计算和分散的内存访问问题.</p>
<p>在阅读完本篇论文并做完课上的论文发表后,想着心思都花了不如整理下来方便以后翻看.</p>
<p>在阅读论文和准备论文发表时也查阅了许多文献, 文中可能有抄录的地方, 但是现在也很难找到原文链接.希望原作者理解.</p>
<p>可能有理解或表述不当的地方, 欢迎大家指正.</p>
<p>论文链接:<a href="https://proceedings.mlsys.org/paper_files/paper/2023/hash/a10deb4d5227a8ea307ea8ff3cb712f4-Abstract-mlsys2023.html" rel="nofollow">Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning</a></p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer">Copyright © <span id="year"></span><a href="https://CX9898.github.io"> CX98的博客 </a>
<p>
<span id="runday"></span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a>
</p>

<script>
if(""!=""){
    var now=new Date();
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("year").innerHTML=now.getFullYear();
    if(""!=""){document.getElementById("runday").innerHTML=" • "+"网站运行"+diffDay+"天"+" • ";}
    else{document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";}
}
</script>
</div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n\n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","CX9898/CX9898.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
