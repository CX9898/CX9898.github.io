<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>CX98的博客</title><link>https://CX9898.github.io</link><description>HPC练习生, Git:https://github.com/CX9898</description><copyright>CX98的博客</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://cx9898.github.io/img/CX.jpg</url><title>avatar</title><link>https://CX9898.github.io</link></image><lastBuildDate>Wed, 03 Jul 2024 07:53:55 +0000</lastBuildDate><managingEditor>CX98的博客</managingEditor><ttl>60</ttl><webMaster>CX98的博客</webMaster><item><title>基于行分解的GPU稀疏矩阵乘法</title><link>https://CX9898.github.io/post/ji-yu-xing-fen-jie-de-GPU-xi-shu-ju-zhen-cheng-fa.html</link><description>![论文封面](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/论文封面.png)&#13;
&#13;
# [论文笔记]基于行分解的GPU稀疏矩阵乘法&#13;
&#13;
**A Row Decomposition-based Approach for Sparse Matrix Multiplication on GPUs**&#13;
&#13;
论文于2024年发表在PPoPP '24: Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming.&#13;
&#13;
稀疏矩阵稠密矩阵乘法(SpMM)和采样稠密-稠密矩阵乘法(SDDMM)是各种计算领域中重要的稀疏核. 论文提出了一种**基于行分解(RoDe)的方法来优化GPU上的两个内核**.使用了标准的压缩稀疏行(CSR)格式. 具体来说,**RoDe将稀疏矩阵行划分为规则部分和残差部分, 分别充分优化它们的计算.还设计了相应的负载均衡和细粒度流水线技术**.与最先进的替代方案相比, Rode的SpMM内核最高达到7.86倍加速, SDDMM内核最高达到8.99倍加速.&#13;
&#13;
***&#13;
&#13;
## 图形处理器(GPU)&#13;
&#13;
物理上, **GPU是由一组流式多处理器(SMs)组成. GPU内核中, 线程(threads)被分成很多个线程块(thread blocks), 在每个线程块(thread blocks)内连续的32个线程组成一组, 称为线程束(warp)**.&#13;
&#13;
&gt; 线程束(warp)是SM中的最小执行单位. 在一个线程束中, 所有线程按照单指令多线程(SIMT)的方式执行, 也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分, 但是处理的数据为私有的数据. 如果一个 warp 内的线程产生分支,该 warp 将执行每一个分支路径, 同时禁用不在该路径上的线程.&#13;
&#13;
![从逻辑角度和硬件角度描述了CUDA编程模型对应的组件. 图片来自谭升的博客](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/从逻辑角度和硬件角度描述了CUDA编程模型对应的组件.图片来自谭升的博客.png)&#13;
&#13;
从逻辑角度和硬件角度描述了CUDA编程模型对应的组件. 图片来自谭升的博客.&#13;
&#13;
当内核在GPU上开始执行时, 线程块以最大化并行性和最小化资源冲突的方式分配给可用的SMs. 在每个SM中, GPU调度器进一步将分配的线程块划分为warp. 每个warp由warp调度器(warp scheduler) 安排执行, 一旦选择执行某个warp, SM的指令调度程序接收这个 warp 的指令, 并将其分配给SM中的执行单元, 以便执行这些指令. GPU可以通过在SM内调度warp来隐藏指令的延迟.&#13;
&#13;
&gt; 每个SM上有非常多的执行单元. 如果某个warp中的线程需要等待数据或发生分支时, warp调度器可以选择其他准备好执行的warp来执行. 利用其他warp的执行来保持执行单元的忙碌状态, 有效地隐藏了指令延迟, 提高GPU的利用率.&#13;
&#13;
GPU有一个大但是高延迟的全局内存, 所有SMs都可以访问它. 一个L2缓存由所有SMs共享, L1缓存位于每个SM的本地.  一个线程块内的所有线程都可以访问同一块共享内存, 并且每个线程都有本地寄存器.&#13;
&#13;
&gt; 同一个线程块内的线程除了都可以访问同一块共享内存外, 在同一个线程束中的线程还可以通过 shuffle 指令进行通信. 通过 suffle 指令, 两个线程间可以相互访问对方的寄存器, 并且延迟极低, 不消耗内存.&#13;
&#13;
当一个warp中的多个线程访问连续的全局内存位置时, GPU可以将这些访问合并到一个事务中, 以提高效率.  每个线程还可以使用单个向量内存指令加载多个数据(例如, 一次加载4个浮点数)&#13;
&#13;
***&#13;
&#13;
## Sparse-Matrix Dense-Matrix Multiplication&#13;
&#13;
稀疏-稠密矩阵乘法(SpMM)是线性代数中的常见操作, 在科学计算, 机器学习等领域有着广泛的应用.&#13;
&#13;
**稀疏矩阵在存储上通常使用特殊的数据结构来优化内存使用和计算效率**，因为它们包含大量的零元素. 例如, 压缩稀疏行(Compressed Sparse Row, CSR), 压缩稀疏列(Compressed Sparse Column, CSC), 坐标列表(Coordinate List, COO)等. **论文中采用按行压缩的CSR表示**, 这是稀疏矩阵中使用最广泛的数据结构之一.&#13;
&#13;
![稀疏矩阵和对应的CSR表示](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/稀疏矩阵和对应的CSR表示.png)&#13;
&#13;
稀疏矩阵和对应的CSR表示.&#13;
&#13;
矩阵只储存非零元素, 行指针数组`row_ptr`中的每个元素指向值数组中对应行的第一个非零元素的位置，列索引数组`col_idx`存储了非零元素的列位置，而值数组`values`则存储了所有非零元素的数值.&#13;
&#13;
&gt; 例如, 取第4行的所有元素, 并获取对应列号:&#13;
&#13;
```C++&#13;
int row = 4;&#13;
for(int idx = row_ptr[row]; idx &lt; row_ptr[row + 1]; ++idx){ // idx : 7, 8&#13;
    const auto val = values[idx]; // val : h, i&#13;
    const int col = col_idx[idx]; // col : 2, 4&#13;
}&#13;
```&#13;
&#13;
***&#13;
&#13;
## Sampled Dense-Dense Matrix Multiplication&#13;
&#13;
采样稠密-稠密矩阵乘法(SDDMM)具有一个稀疏矩阵和两个稠密矩阵作为输入,一个稀疏矩阵作为输出. 采样指的是对两个稠密矩阵乘法的结果矩阵中随机保留一些元素. **SDDMM计算两个稠密输入矩阵的乘积, 但仅根据输入稀疏矩阵对应的非零位置处进行计算, 并与对应位置的非零权重值相乘**.&#13;
&#13;
&gt; 关于SDDMM具体介绍可以参考另一篇笔记: [CX98：[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法](https://zhuanlan.zhihu.com/p/699010780)&#13;
&#13;
***&#13;
&#13;
## 分析非零分布&#13;
分析来自 SuiteSparse 集合的稀疏矩阵的每一行中的非零分布.&#13;
&gt; SuiteSparse 是一个大型的稀疏矩阵集合, 它包含了许多实际应用中的稀疏矩阵.&#13;
&#13;
![左图显示每个矩阵的平均行长(每行非零元素的数量). 中间图绘制了每个特定行的长度分布. 右图显示每个矩阵内的行数分布. 数据集来自SuiteSparse](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/数据集SuiteSparse.png)&#13;
&#13;
左图显示每个矩阵的平均行长(每行非零元素的数量). 中间图绘制了每个特定行的长度分布. 右图显示每个矩阵内的行数分布. 数据集来自SuiteSparse.&#13;
&#13;
得出**数据集中稀疏矩阵的三个属性:**&#13;
- **大多数矩阵的平均行长都小于32(红线)**&#13;
- **大部分行都很短(84%的行长度小于32), 但也有一些行长度达到几千甚至上万**&#13;
- **很多矩阵主要由短行组成, 包含少量的长行**&#13;
&#13;
属性①导致难以充分利用GPU的计算或内存访问资源, 减少了一些可优化的空间.&#13;
&#13;
&gt; 上面提到SM的最小执行单位是线程束(warp), 一个warp由32个线程组成, 32个线程同时同步执行指令. 按照一个线程计算一个非零元素, 一行中的非零元素少于32个导致有很多线程处于空闲状态.&#13;
&#13;
属性②表示对长行进行优化是有益的. 可以用向量内存指令和更多的线程来优化这些行.&#13;
&#13;
&gt; Vector Memory Instructions(向量内存指令)是GPU编程中的一种技术, 它允许单个线程使用一条指令从内存中加载多个数据元素(例如一次性加载4个浮点数). 这种技术可以提高内存访问的效率，因为它减少了访问内存的次数, 同时利用了GPU的SIMD(单指令多数据)特性.&#13;
&#13;
属性③指出, 对于很多矩阵, 属性①和属性②是共存的. 意味着应该在一次计算中同时优化它们. 并且长短行的数量差距很大, 需要积极的负载均衡.&#13;
&#13;
**结论: 需要同时优化长,短行, 避免优化不彻底, 同时负载均衡也很重要.**&#13;
&#13;
***&#13;
&#13;
## 分析计算管道&#13;
&#13;
GPU上进行矩阵求解时, 通常利用分块(tiling)技术, 将稀疏矩阵和稠密矩阵划分为多个块(tile), 每个块(tile)只包含矩阵的一部分行和列. 随后迭代计算每个块, 并将其结果累加到结果块中.&#13;
&#13;
&gt; 分块(tiling): 指的是将大数据集(如矩阵)划分为较小的块(tile)(或称为瓦片). 这些小块的大小通常与GPU的共享内存大小相匹配，以便可以完全加载到共享内存中. 是常用的优化技术.&#13;
&#13;
![常见的稀疏稠密矩阵乘法计算流水线](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/常见的稀疏稠密矩阵乘法计算流水线.png)&#13;
&#13;
常见的稀疏稠密矩阵乘法计算流水线.&#13;
&#13;
如图所示GPU上稀疏稠密矩阵乘法主要分为三个阶段: 加载稀疏块, 加载稠密块和计算.&#13;
&#13;
由于要利用稀疏块内的列索引来加载稠密块, 最终的计算则需要完成对稠密块的加载, 这三个阶段表现出紧密的数据依赖关系. 为了使稀疏块在线程之间重用, 一般将其放在共享内存中, 由不同的线程加载稀疏块的不同部分, 这就需要线程间的同步操作`syncthreads`.&#13;
&#13;
但上述**同步限制了编译器的优化空间, 并且稠密块中更大的数据量使得在加载稠密块时, 管道经常会出现停顿**. 通过 Nvidia Nsight Compute 分析得出, warp 需要花费几倍的时间周期的延迟在等待稠密块数据的计算指令上. **如果能在内存访问的延迟内执行更多的指令或更多的计算, 对这部分会有很好的提速效果**.&#13;
&#13;
&gt; Warp准备好执行下一条指令所需的时钟周期称为延迟, warp 调度器可以在这些延迟周期内, 向其他 warp 发出指令, 从而实现资源的充分利用, 这种方法被称为延迟隐藏.&#13;
&#13;
***&#13;
&#13;
## RoDe&#13;
&#13;
行分解(RoDe)方法旨在使用CSR表示法在GPU上加速稀疏矩阵乘法. 主要分为三个部分: 行分解(Row Decomposition), 块分割(Block  Split)和子块流水线(Sub-block Pipeline).&#13;
&#13;
![RoDe方法工作流程](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/RoDe方法工作流程.png)&#13;
&#13;
RoDe方法工作流程.&#13;
&#13;
**先将稀疏矩阵的行分解(Row Decomposition)为块部分(a)和残差部分(b), 分别进行处理. 将块分割(Block Split)方法应用于块部分以实现负载平衡. 最后构建子块流水线, 在连续的同步之间执行更多的操作, 以更好的隐藏内存访问延迟.**&#13;
&#13;
***&#13;
&#13;
### 行分解(Row Decomposition)&#13;
&#13;
根据分析稀疏矩阵的非零分布得出的属性③, **将长短行分开进行处理**. 但并没有将长短行直接分成两组, 而是**把每行都分成块部分(block part)和残差部分(residual part). 将这两部分分开组合, 独立进行处理**.&#13;
&#13;
![行分解的一个例子](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/行分解的一个例子.png)&#13;
&#13;
行分解的一个例子. (a)为块部分(非零元素数量为32的倍数), (b)为残差部分(将一行元素数量与32取余,余数为残差部分). 虚线方框表示在实际实现中不需要储存.&#13;
&#13;
&gt; 例如第二行有104个非零元素, 将元素数量与32取余, 104%32 = 8. 则第二行最后8个非零元素为残差部分, 前96个元素为块部分.&#13;
&#13;
为每个部分使用两个辅助数组来实现行分解:&#13;
- rowIndexes: 将每个部分的行索引储存在稀疏矩阵中.&#13;
- StartOffset: 储存每个部分中每行起始元素的索引.(在实际实现中, 可以在运行时被推算出来).&#13;
&#13;
通过将行分解为块部分和残差部分, 可以分别对属性①和②进行优化. 使用具有较大线程块的内核处理块部分, 使用具有较小线程块的内核处理剩余部分.&#13;
&#13;
&gt; 行分解使得每行的两个部分可以单独处理, 从而提高了GPU资源的利用率.&#13;
&#13;
***&#13;
&#13;
### 块分割(Block  Split)&#13;
&#13;
属性②和③说明单个矩阵中, **不同行的长度差异很大, 需要良好的负载均衡**.&#13;
&#13;
在行分解的基础上引入了块分割(block split)方法. **将块部分的每一行划分为固定长度的分段, 数组`RowIndices`记录每个分段的行索引.`StartOffsets`储存每个分段中第一个元素的起始索引**.&#13;
&#13;
![块分割的例子](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/块分割的例子.png)&#13;
&#13;
块分割的例子. `RowIndices`储存行索引, `StartOffsets`储存对应的起始下标.&#13;
&#13;
块分割后, 每行的元素被分割成连续的分段,将每个分段视为一个独立的行进行处理, 并且使用`atoMicAdd`指令将每个段的结果累积到最终输出中.&#13;
&#13;
**每个分段中连续的数据储存有利于GPU上线程的联合内存访问. 并且每个分段都有相同数量的元素, 更容易实现负载均衡**. 生成这两个数组只需要花费很小的预处理操作.&#13;
&#13;
***&#13;
&#13;
### 子块流水线(Sub-block Pipeline)&#13;
&#13;
由于全局内存访问的长延迟, 常见的负载计算流水线可能会收到性能限制. 论文引入了子块流水线方法, 用以更好地重叠共享内存访问, 全局内存访问和计算. 原理是**在内存访问的延迟内, 发出尽可能多的指令和执行尽可能多的计算**.&#13;
&#13;
![子块流水线](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/子块流水线.png)&#13;
&#13;
子块流水线. 每个块的左边缘表示指令发出的时间. 例如ld dn[1]在comp[0]之前发出.&#13;
&#13;
为了确保数据重用, 选择将稀疏块加载到共享内存中, 而将密集块直接加载到寄存器中. 各个线程加载一部分稀疏块,并且合并到共享内存中. 由于加载密集块时需要稀疏块的列标, 因此在加载密集块前的同步操作是不可避免的.&#13;
&#13;
对于图中的例子, 将密集块划分为两个子块dn[0]和dn[1], 首先加载dn[0], 然后在加载子块dn[1]的同时计算子块[0]. 在加载稀疏块sp[2]和sp[3]时同时计算子块[1].&#13;
&#13;
**这个方法构建了一个更精细的流水线, 将加载内存和计算交错进行, 有效扩展了指令的重叠空间, 更好的使用计算资源**. 再次通过 Nvidia Nsight Compute 分析, 花费的等待时间周期明显减少.&#13;
&#13;
&gt; 扩展了指令的重叠空间意味着更能利用warp调度器的调度能力, 实现资源的充分利用, 能够更好地隐藏延迟.&#13;
&#13;
***&#13;
&#13;
## RoDe性能测试&#13;
&#13;
- 环境: CUDA 11.2  Nvidia A100-PCIE-40GB GPU. CUDA代码使用带有-O3标志的NVCC 11.1进行编译&#13;
- 数据集: SuiteSparse集合中选择900多个矩阵, 这些矩阵至少有10K行,10K列和100K个非零. 矩阵来自不同的应用领域, 包括科学计算, 图形处理, 图形挖掘等, 并且包含广泛的稀疏模式&#13;
- 比较方法 : 与Nvidia cuSPARSE库, ASpT和Sputnik进行比较. 对内核执行时间和与处理时间分别进行分析. 所有测试都连续运行10次得到平均值&#13;
&#13;
***&#13;
&#13;
### SpMM内核&#13;
&#13;
![SpMM性能结果](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/SpMM性能结果.png)&#13;
&#13;
SpMM性能结果. 图中每个点表示连续5个矩阵集的平均GFLOP.&#13;
&#13;
- cuSPARSE基线上的加速：RoDe实现了特定情况下高达32.17倍的加速. 几何平均值为1.91倍.&#13;
- Sputnik基线上的加速：RoDe实现了特定情况下高达198.51倍的加速. 几何平均值为1.83倍.&#13;
- ASpT基线上的加速：RoDe实现了特定情况下高达8.02倍的加速. 几何平均值为1.45倍.&#13;
&#13;
***&#13;
&#13;
### SDDMM内核&#13;
&#13;
![SDDMM性能结果](/img/[论文笔记]基于行分解的GPU稀疏矩阵乘法/SDDMM性能结果.png)&#13;
&#13;
SDDMM性能结果. 图中每个点表示连续5个矩阵集的平均GFLOP.&#13;
&#13;
与ASP相比, FP32(K=32)和FP32(K=128)的两种情况下, RoDe实现了高达8.99倍和8.80倍的加速, 几何平均值分别为1.54倍和1.44倍.&#13;
&#13;
***&#13;
&#13;
## 结语&#13;
&#13;
论文首先分析了稀疏矩阵的跨行分布特性, 总结出将长短行分别进行优化和负载均衡的重要性. 利用行分解技术解耦了块部分和残差部分, 引入了新的负载均衡和更细致的计算流水线技术对稀疏矩阵乘法进行进一步优化.&#13;
&#13;
可能有理解或表述不当的地方, 欢迎大家指正.&#13;
&#13;
论文链接: [A Row Decomposition-based Approach for Sparse Matrix Multiplication on GPUs](https://dl.acm.org/doi/10.1145/3627535.3638470)&#13;
。</description><guid isPermaLink="true">https://CX9898.github.io/post/ji-yu-xing-fen-jie-de-GPU-xi-shu-ju-zhen-cheng-fa.html</guid><pubDate>Tue, 02 Jul 2024 11:03:05 +0000</pubDate></item><item><title>用于高性能机器学习的抽样稠密矩阵乘法</title><link>https://CX9898.github.io/post/yong-yu-gao-xing-neng-ji-qi-xue-xi-de-chou-yang-chou-mi-ju-zhen-cheng-fa.html</link><description>![论文封面](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/论文封面.png)&#13;
&#13;
# [论文笔记]用于高性能机器学习的抽样稠密矩阵乘法&#13;
&#13;
**Sampled Dense Matrix Multiplication for High-Performance Machine Learning**&#13;
&#13;
论文于 2018 年发表在 IEEE 25th International Conference on High Performance Computing (HiPC).&#13;
&#13;
论文详细介绍了**采样稠密-稠密矩阵乘法(SDDMM)** 作为许多机器学习因子分析算法（如 Alternating Least Squares (ALS)、Latent Dirichlet Allocation (LDA)、Sparse Factor Analysis (SFA) 和 Gamma Poisson (GaP)）的核心组件. **SDDMM 需要计算两个输入稠密矩阵的乘积，但仅在结果矩阵中对应于第三个输入稀疏矩阵的非零位置处进行计算.** 论文还介绍了 cuSDDMM，这是一个多节点 GPU 加速的 SDDMM 实现，相对于当前最佳的 GPU 实现（在 BIDMach 机器学习库中）具有最高4.6倍的加速效果.&#13;
&#13;
***&#13;
&#13;
## Sampled Dense-Dense Matrix Multiplication&#13;
&#13;
采样稠密-稠密矩阵乘法(SDDMM)具有一个稀疏矩阵和两个稠密矩阵作为输入,一个稀疏矩阵作为输出. 采样指的是对两个稠密矩阵乘法的结果矩阵中随机保留一些元素. **SDDMM计算两个稠密输入矩阵的乘积, 但仅根据输入稀疏矩阵对应的非零位置处进行计算, 并与对应位置的非零权重值相乘.**&#13;
&gt; 在几种ML算法中(如交替最小二乘(ALS)), SDDMM内核在计算上占主导地位(占总执行时间的65%), 优化SDDMM算法可以提高几种ML算法的性能.&#13;
&#13;
![SDDMM示例:在稀疏矩阵S中非零位置的元素累积稠密矩阵A和B的乘积,生成输入稀疏矩阵P](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/SDDMM示例_在稀疏矩阵S中非零位置的元素累积稠密矩阵A和B的乘积,生成输入稀疏矩阵P.png)&#13;
&#13;
SDDMM示例:在稀疏矩阵S中非零位置的元素累积稠密矩阵A和B的乘积,生成输入稀疏矩阵P.&#13;
&#13;
&gt; 在矩阵乘法中处理大规模稠密矩阵时, 计算复杂度和内存开销是主要挑战. 采样技术通过选择矩阵中的一部分元素进行计算, 从而有效降低计算量.&#13;
&#13;
SDDMM可以使用A和B之间的高效稠密矩阵乘法(DGEMM)执行,然后提取采样元素. 但是这样会产生大量不必要的计算. **通过只执行与非零元素对应的计算,计算复杂度可以从O(K.M.N)降低到O(K.nnz)(nnz:number of non-zero)**.&#13;
&#13;
与研究较多的优化SpMV(稀疏矩阵向量积)问题相比, SDDMM具有一个输入稀疏矩阵和两个输入稠密矩阵, 因此在**为GPU设计有效并行实现时,需要考虑更多的数据访问**. 而且与内存带宽严重受限的SpMV不同的是, **SDDMM的每个输入稀疏矩阵的元素都会乘以两个稠密输入矩阵的向量的点积, 具有很多可以合并的内存访问**. 因此与SpMV相比, SDDMM显著提高了Roofline性能极限.&#13;
&#13;
&gt; Roofline模型是关注算力和带宽来研究和分析程序运行的瓶颈, 具体可以参考这个文章: [Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)&#13;
&#13;
***&#13;
&#13;
## cuSDDMM&#13;
&#13;
论文介绍的cuSDDMM是一种SDDMM的多节点GPU加速实现. 通过分析SDDMM的数据重用特征给出了两种解决方案. SM-SM(Shared memory-Shared memory)方案和SM-L2(shared memory-L2 cache)方案.&#13;
&#13;
&gt; 在GPU加速优化中, 对于数据重用的情况, 利用共享内存来加速是一个很常用方法. 共享内存访问的低延迟和高带宽可以很好地优化数据重用. 对于共享内存的详细介绍可以查看这个博客: [【CUDA 基础】5.1 CUDA共享内存概述](https://face2ai.com/CUDA-F-5-1-CUDA%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E6%A6%82%E8%BF%B0/)&#13;
&#13;
***&#13;
&#13;
### SM-SM方案&#13;
&#13;
SM-SM(Shared memory-Shared memory)方案通过将A和B两个矩阵加载到共享内存来消除未合并的全局内存访问.&#13;
&#13;
&gt; 共享内存的延迟大约比全局内存访问低100倍&#13;
&#13;
但由于共享内存大小的限制, A片和B片的体积应该小于共享内存容量, 这限制了每个线程块的工作量.&#13;
&#13;
![SM-SM方案:每个线程块根据其共享内存容量将A片和B片加载到GPU的共享内存中](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/SM-SM方案.png)&#13;
&#13;
SM-SM方案:每个线程块根据其共享内存容量将A片和B片加载到GPU的共享内存中.&#13;
&#13;
如果稀疏矩阵的密度高, 那么A和B矩阵都能得到很好的重用, 这个方案会有很好地表现. 但由于共享内存大小的限制, A片和B片能存放在共享内存上的体积并不多. 每个稀疏矩阵的一个瓦片(tiled)分配给一个CUDA线程块, 然而A片和B片的大小限制了瓦片(tiled)的大小. 如果瓦片中没有包含足够的工作量(非零数据). 许多线程就会处于空闲状态, 从而降低性能.&#13;
&#13;
实验结果显示如果稀疏矩阵密度≥5%时, 该方案明显优于 SM-L2 方案.&#13;
&#13;
![使用SM-L2方案和SM-SM方案计算两个矩阵(75000×75000)和(100000×100000), 测试不同密度下的GFLOPS性能](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/使用SM-L2方案和SM-SM方案计算两个矩阵(75000×75000)和(100000×100000),%20测试不同密度下的GFLOPS性能.png)&#13;
&#13;
&gt; GFLOPS（Giga Floating Point Operations Per Second）是一个衡量计算设备性能的指标, 常用于描述CPU和GPU的性能. 它表示每秒能够执行的十亿次浮点运算次数.&#13;
&#13;
***&#13;
&#13;
### SM-L2方案&#13;
&#13;
SM-L2(shared memory-L2 cache)方案将其中一个稠密矩阵储存在共享内存中, 再利用L2缓存进行数据重用, 根据L2缓存容量来调整用于在一个线程块上计算的矩阵的大小.&#13;
&#13;
![SM-L2 方案的分块和不分块的版本](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/SM-L2 方案的分块和不分块的版本.PNG)&#13;
&#13;
SM-L2 方案的分块和不分块的版本. (a) 非分块: 矩阵A加载到共享内存中, 矩阵B依赖L2缓存进行数据重用. (b) (c) 分块: 将B矩阵分成两个瓦片分别进行计算. 在单个GPU上时,块1和块2按顺序执行, 在多GPU节点上时并行执行.&#13;
&#13;
&gt; 分块(tiling):  指的是将大数据集（如矩阵）划分为较小的块(或称为瓦片). 这些小块的大小通常与GPU的共享内存大小相匹配，以便可以完全加载到共享内存中. 是常用的优化技术.&#13;
&#13;
在实际应用中, 实值矩阵通常表现出一种幂律结构特征, 即大多数行（或列）包含的非零元素数量很少, 而只有少数行（或列）包含大量的非零元素. 通过分块（tiling）处理数据时, 分块会增加在某个块(tiled)中出现空行的概率. 每个CUDA线程块在处理一个块(tiled)时，会将A矩阵中的连续行（即使是未使用的行）加载到共享内存中, 这样会导致某个时间步骤中可用的工作量受到限制. 为了缓解这个问题, **为每个块(tiled)维护一个'活跃行'的列表, 只将需要计算的行加载到共享内存中进行处理**.&#13;
&#13;
在SM-L2方案的实现中, 每个线程块使用SM上可用共享内存的一半, 只有两个线程块可以同时活动. 为了最大化占用, 每个线程块分配1024个线程(单个线程块所能分配的最大线程数). 但是因为输入矩阵非常稀疏, 导致每个线程块能处理的元素少于1024个, 如果按照一个元素分配给一个线程计算, 则会有一些线程无事可做. 为了解决这个问题, 可以**让单个元素分配给多个线程进行计算, 增加并行度. 这种情况则需要归约操作来合并多个线程计算的结果**, 使用warp shuffle可以高效地完成这个工作.&#13;
&#13;
&gt; warp shuffle作用在一个线程束内, 允许两个线程间相互访问对方的寄存器, 并且延迟极低, 不消耗内存. shuffle指令是线程束内线程通讯的极佳方式. 关于shuffle 具体可以参考这个博客: [【CUDA 基础】5.6 线程束洗牌指令](https://face2ai.com/CUDA-F-5-6-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4/)&#13;
&#13;
***&#13;
&#13;
## cuSDDMM在多GPU上的可扩展性&#13;
&#13;
CPU上的DRAM内存通常远远大于GPU上的全局内存容量. 用GPU进行加速计算的同时, 单个GPU的全局内存通常不足以容纳大型问题(如更大的矩阵), 这激发了对多GPU SDDMM解决方案的需求.&#13;
&#13;
&gt; 多GPU的利用主要为了解决单个GPU容量不足的问题. 多个GPU解决问题时, GPU之间需要进行数据通信和同步，存在一定的通信开销，特别是在数据量较大时可能会成为性能瓶颈.&#13;
&#13;
根据上节提到SM-L2 方案. **将输入矩阵分为多个分块,每个块依次处理. 多节点方案中,可以在多台机器上并行启动内核, 从而可以同时处理多个块**.&#13;
&#13;
然而, 在不同节点上平均划分整个列可能会导致显著的负载不均衡. 例如一个稀疏矩阵中大多非零元素在左半边, 从中间平均划分导致其中一个节点计算量过大, 又由于'木桶效应', 计算瓶颈将会在出现在这个节点.&#13;
&#13;
![稀疏矩阵: 蓝色方块代表非零元素,白色方块代表0](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/稀疏矩阵_蓝色方块代表非零元素,白色方块代表0.PNG)&#13;
&#13;
稀疏矩阵: 蓝色方块代表非零元素,白色方块代表0.&#13;
&#13;
为了缓解这个问题, 采用了一种非对称的分区技术,根据稀疏矩阵S中每列非零的数量, 将S分成多个ID分块, 使每个分块都有相似的工作量. 其中稠密矩阵A或B的其中一个也被划分到不同的节点上, 另一个矩阵则被所有节点共享.&#13;
&#13;
***&#13;
&#13;
## cuSDDMM性能测试&#13;
&#13;
- 实验在 NAVIDIA Tesla P100 GPU机器上运行. 具有56 SMs, 64 cores/MP, 16 GB 全局内存, 1328MHz 时钟频率和4MB L2缓存.&#13;
- CPU节点为 Intel(R) Xeon(R) CPU E5-2680 V4(28核).&#13;
- 图数据集来自SNAP和GraphChallenge.&#13;
&#13;
- 与BIDMesh库的SDDMM GPU实现进行比较. 加速效果最多达到4.6倍.&#13;
&#13;
![在Tesla P100 GPU上使用 a)K=32, b)K=128, c)K=512时的性能(GFLOPS), 蓝色代表BIDMach, 红色代表基于模型的cuSDDMM, 橙色代表使用cuSDDMM的穷举搜索](/img/[论文笔记]用于高性能机器学习的抽样稠密矩阵乘法/测试结果.png)&#13;
&#13;
在Tesla P100 GPU上使用 a)K=32, b)K=128, c)K=512时的性能(GFLOPS), 蓝色代表BIDMach, 红色代表基于模型的cuSDDMM, 橙色代表使用cuSDDMM的穷举搜索.&#13;
&#13;
***&#13;
&#13;
## 结语&#13;
&#13;
目前针对SDDMM内核中GPU不规则访问的问题的优化并不多.cuSPARSE库中缺乏优化的SDDMM函数是论文的动机. 对于SDDMM内核主要利用共享内存和L2缓存来合并全局内存访问.&#13;
&#13;
可能有理解或表述不当的地方, 欢迎大家指正.&#13;
&#13;
论文链接: [Sampled Dense Matrix Multiplication for High-Performance Machine Learning](https://ieeexplore.ieee.org/abstract/document/8638042)&#13;
。</description><guid isPermaLink="true">https://CX9898.github.io/post/yong-yu-gao-xing-neng-ji-qi-xue-xi-de-chou-yang-chou-mi-ju-zhen-cheng-fa.html</guid><pubDate>Tue, 02 Jul 2024 10:58:21 +0000</pubDate></item><item><title>深度学习中N:M稀疏权重的高效GPU内核</title><link>https://CX9898.github.io/post/shen-du-xue-xi-zhong-N-M-xi-shu-quan-zhong-de-gao-xiao-GPU-nei-he.html</link><description>![论文封面](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/论文封面.png)&#13;
&#13;
# [论文笔记]深度学习中N:M稀疏权重的高效GPU内核&#13;
&#13;
**EFFICIENT GPU KERNELS FOR N:M-SPARSE WEIGHTS IN DEEP LEARNING**&#13;
&#13;
论文于2023年发表在Sixth Conference on Machine Learning and Systems · Miami (MLSys 23).&#13;
&#13;
在深度学习领域中,N:M稀疏性越来越受欢迎. 但因为缺乏针对各种稀疏比的通用GPU kernel库,论文介绍了一个**高效GPU kernel库: nmSPARSE. 用于具有N:M稀疏权重的神经网络的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).**&#13;
&#13;
论文主要探讨针对N:M稀疏性的稀疏矩阵-向量乘法和稀疏矩阵-矩阵乘法的GPU加速. 下面先介绍什么是权重剪枝和N:M稀疏性.&#13;
&#13;
***&#13;
&#13;
## Weight Pruning&#13;
&#13;
为了减少深度神经网络(DNN)的模型大小并加速模型推理, 权重剪枝(Weight Pruning)算法在学术界和工业界得到广泛的研究.&#13;
&#13;
**权重剪枝的目的是找到并去除对模型精度影响不大的冗余权重. 从而直接减少了模型的内存占用和计算规模.**&#13;
&#13;
![如图所示,图中占比重要的权重被保留,冗余的权重被去除.在保留模型精度的同时降低了计算规模](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/稀疏剪枝.png)&#13;
&#13;
如图所示, 图中占比重要的权重被保留, 冗余的权重被去除. 在保留模型精度的同时降低了计算规模.&#13;
&#13;
&gt; 通过调整权重剪枝(Weight Pruning)算法可以将权值修剪为N:M稀疏模式, 随着权值被修剪为稀疏后, DNN推理中最频繁和耗时的稠密矩阵-向量乘法(GEMV)和稠密矩阵-矩阵乘法(GEMM)变为了具有N:M稀疏性的稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM). 那么什么是N:M稀疏性呢?&#13;
&#13;
***&#13;
&#13;
## N:M sparsity&#13;
&#13;
N:M稀疏性可以为深度学习提供高模型精度和计算效率.&#13;
&#13;
**N:M稀疏性本质上对非零权重施加了平衡分布,具体为每连续的M个权值中,只有N个权值不为零.**&#13;
&#13;
![三种稀疏模式](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/三种NM稀疏模式.png)&#13;
&#13;
图上展示了三种稀疏模式, 元素型, 矢量型和块型. 图中蓝色方块代表非零值, 白色方块代表0. 上面一列是无约束分布的数据, 下面一列是N:M平衡分布的数据. 图中稀疏比为1:4. 左下角的图表示垂直的每四个权值中有一个非零权值.&#13;
&#13;
&gt; 注意,N:M分布是沿着矩阵乘法中的降维k分布的&#13;
N:M稀疏性只限制了非零元素的局部分布,每个M大小的窗口中的分布不受限制.所以对模型精度的影响很小.同时N:M稀疏性在GPU上实现高效并行执行方面有很大的潜力.&#13;
&#13;
***&#13;
&#13;
## N:M sparsity压缩表示&#13;
&#13;
稀疏矩阵有各种压缩表示格式,如CSC,CSR,COO等.&#13;
&#13;
论文中给出专用于nmSPARSE中N:M稀疏模式的压缩表示格式.&#13;
&#13;
![EW-/VW-/BW-N:M sparsity的数据压缩表示](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/NM稀疏模式的压缩表示格式.png)&#13;
&#13;
EW-/VW-/BW-N:M sparsity的数据压缩表示.&#13;
&#13;
如图,将非零数据按竖直方向压缩后,创建一个大小相同的位置数组记录对应非零数据在原矩阵中M窗口中的位置索引. 并记录稀疏模式和稀疏比.&#13;
&#13;
***&#13;
&#13;
## nmSPARSE库&#13;
&#13;
论文介绍的nmSPARSE是一个高效的GPU kernel库,用于N:M稀疏权重的神经网络中的两种基本操作:稀疏矩阵-向量乘法(SpMV)和稀疏矩阵-矩阵乘法(SpMM).&#13;
&#13;
并基于ASP算法(由Nvidia开发用于生成稀疏网络的开源Pruning库), 做出三个扩展:&#13;
1. 支持任意N:M设置. 意味着支持生成任意N:M稀疏比&#13;
2. 扩展ASP支持nmSPARSE的VW/BW-N:M稀疏模式&#13;
3. 进一步启用分层稀疏比配置&#13;
&#13;
值得一提的是**在实现nmSPARSE的SpMV和SpMM kernel时利用了共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).**&#13;
&#13;
下面介绍什么是共享内存的无冲突访问模式(conflict-free access)和无冲突广播访问模式(conflict-free broadcast access).&#13;
&#13;
***&#13;
&#13;
### 共享内存:无冲突访问(conflict-free access)&#13;
&#13;
&gt; 共享内存是一个可以被同时访问的一维地址空间.&#13;
&gt; 共享内存中被分为32个同样大小的存储体(bank)，对应线程束中32个线程.&#13;
&#13;
**在共享内存中当多个地址请求落在相同的内存库的不同地址中时， 就会发生存储体冲突(bank conflict)， 这会导致请求被重复执行, 从而降低带宽.**&#13;
&#13;
![多个线程同时访问同一个bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/多个线程同时访问同一个bank(bank_conflict).png)&#13;
&#13;
多个线程同时访问同一个bank产生bank conflict. 图片来自谭升的博客.&#13;
&#13;
&gt; 这里对于bank conflict不做深入的介绍,具体可以参考这个链接的文章:[共享内存之bank冲突](https://segmentfault.com/a/1190000007533157)&#13;
&#13;
**相反,如果warp中的32个线程的内存访问映射到32个不同的存储体,那么可以同时提供服务,不会产生bank冲突,这种模式也被称为无冲突访问模式(conflict-free access)**&#13;
&#13;
![每个线程都访问不同的bank. 图片来自谭升的博客](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/无冲突访问模式.png)&#13;
&#13;
最理想的访问模式, 每个线程都访问不同的bank. 图片来自谭升的博客.&#13;
&#13;
当然不规则访问的同时访问不同的bank也属于无冲突访问模式(conflict-free access)&#13;
&#13;
![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/不规则访问但也不冲突.png)&#13;
&#13;
不规则访问的无冲突访问模式(conflict-free access). 图片来自谭升的博客.&#13;
&#13;
***&#13;
&#13;
### 共享内存:无冲突广播访问(conflict-free broadcast access)&#13;
&#13;
**当一个warp中的多个线程访问同一个bank,但是使用的是完全相同的地址时,可以通过硬件支持的广播机制来提供服务,不会出现存储体冲突(bank conflict)**.这种访问模式称为无冲突广播访问模式(conflict-free broadcast access).&#13;
&#13;
![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同的bank.png)&#13;
&#13;
&gt; 上图两条红色的线访问了同一个bank1的不同地址, 出现了bank冲突.&#13;
&#13;
![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/两个线程访问相同bank的相同地址.png)&#13;
&#13;
&gt; 如上图, 虽然有两个线程都访问了同一个bank1,但是访问的是完全相同的地址.这种访问模式就称为无冲突广播访问模式(conflict-free broadcast access).&#13;
&#13;
***&#13;
&#13;
### nmSPARSE: SpMV kernel的无冲突访问&#13;
&#13;
1. nmSPARSE的SpMV kernel进一步将每个稀疏列划分为大小为M的子列. &#13;
2. 每个线程计算稀疏矩阵的一个子列的点积.&#13;
3. 因为每M个元素中只有N个非零元素,所以每个线程的工作负载是平衡的.&#13;
4. 稠密向量A则从全局内存加载到共享内存中,并且根据每个子列需要的向量划分到不同的内存块中.&#13;
5. 不同线程只需要访问不同的内存块,消除了bank冲突.&#13;
&#13;
![各个线程访问不同的储存库,例如线程T0访问bank0,线程T1访问bank1](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMV.png)&#13;
&#13;
***&#13;
&#13;
### nmSPARSE: SpMM kernel的无冲突广播访问&#13;
&#13;
nmSPARSE中的SpMM kernel将稠密矩阵A储存在共享内存中,将线程映射到稀疏矩阵B中的列来实现对A中所需元素的访问&#13;
&#13;
![](/img/[论文笔记]深度学习中NM稀疏权重的高效GPU内核/nmSPARSE_SpMM.png)&#13;
&#13;
1. 通过将稠密A矩阵按行储存在共享内存中,如图A矩阵一行有32个元素,一行的元素刚好可以对应储存同在一个bank.&#13;
2. 每个线程负责稀疏B矩阵的每一列.&#13;
3. 由于N:M稀疏模式按列的内部平衡分布,所以每个线程的工作负载是平衡的. &#13;
4. 在同一个warp中的两个线程访问同一块内存中完全相同的地址,可以运用广播机制完美解决这个冲突.&#13;
&#13;
***&#13;
&#13;
## nmSPARSE性能测试&#13;
&#13;
通过与当前最先进的稠密和稀疏库以及DNN编译器进行比较，在操作基准和端到端模型上评估nmSPARSE内核。</description><guid isPermaLink="true">https://CX9898.github.io/post/shen-du-xue-xi-zhong-N-M-xi-shu-quan-zhong-de-gao-xiao-GPU-nei-he.html</guid><pubDate>Tue, 02 Jul 2024 05:13:07 +0000</pubDate></item></channel></rss>